{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAC33tfJDZhe",
        "outputId": "0aa1967f-f51a-4ae0-93a6-ea8f2016ba6a"
      },
      "source": [
        "!pip install transformers==3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n",
            "\r\u001b[K     |▍                               | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 18.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 14.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 13.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 7.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 9.1MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |████                            | 92kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102kB 6.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 133kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 153kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 163kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 174kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 184kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 194kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 204kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 215kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 225kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 235kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 245kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 256kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 266kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 276kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 286kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 296kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 307kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 317kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 337kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 348kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 358kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 368kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 378kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 389kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 399kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 409kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 419kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 430kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 440kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 450kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 460kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 471kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 481kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 491kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 501kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 512kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 522kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 532kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 542kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 552kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 563kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 573kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 583kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 593kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 604kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 614kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 624kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 634kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 645kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 655kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 665kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 675kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 686kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 696kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 706kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 716kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 727kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 737kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 747kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 757kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 31.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3) (20.9)\n",
            "Collecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/82/0e82a95bd9db2b32569500cc1bb47aa7c4e0f57aa5e35cceba414096917b/tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 33.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 35.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3) (1.19.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.95 tokenizers-0.8.0rc4 transformers-3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZJAwfU3utMH"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# Use GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUvd7QKKkYON",
        "outputId": "352b940b-71e8-46f0-cfda-a614955aed71"
      },
      "source": [
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "emUOZPJEutbw",
        "outputId": "75b826ba-7ad8-450f-b833-a8ac13520f6b"
      },
      "source": [
        "df = pd.read_csv(\"Cleaned_tweets.csv\")\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Key Word</th>\n",
              "      <th>Username</th>\n",
              "      <th>User_ID</th>\n",
              "      <th>Datetime</th>\n",
              "      <th>Favorite_count</th>\n",
              "      <th>Geo</th>\n",
              "      <th>Coordinates</th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>Unnamed: 10</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>704</td>\n",
              "      <td>ass</td>\n",
              "      <td>DeborahParr</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>11-11-2020 06:56</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>He’d have my phone wedged up his ass sideways.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1915</td>\n",
              "      <td>boobies</td>\n",
              "      <td>MaxZorin85</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>11-11-2020 07:35</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>Yep 100% agree and the same with severine in s...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2856</td>\n",
              "      <td>eat pussy</td>\n",
              "      <td>PRISJ1_</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>11-11-2020 10:36</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>Stop having sex with men that won’t eat your p...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2163</td>\n",
              "      <td>Breast Man</td>\n",
              "      <td>Teresamckenzy1</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>10-11-2020 20:52</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>When you see a sad man, just give him breast t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2852</td>\n",
              "      <td>eat pussy</td>\n",
              "      <td>sj__vazquez</td>\n",
              "      <td>1.330000e+18</td>\n",
              "      <td>11-11-2020 10:42</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>We can't be together if you don't eat pussy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0    Key Word        Username  ...  Unnamed: 9 Unnamed: 10  Unnamed: 11\n",
              "0         704         ass     DeborahParr  ...         NaN         NaN          NaN\n",
              "1        1915     boobies      MaxZorin85  ...         NaN         NaN          NaN\n",
              "2        2856   eat pussy         PRISJ1_  ...         NaN         NaN          NaN\n",
              "3        2163  Breast Man  Teresamckenzy1  ...         NaN         NaN          NaN\n",
              "4        2852   eat pussy     sj__vazquez  ...         NaN         NaN          NaN\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWOJmRUHxMf4",
        "outputId": "f3530a77-5590-4908-b831-1ff7ad1642f6"
      },
      "source": [
        "# Displaying Class distribution\n",
        "df['Label'].value_counts(normalize = True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    0.547447\n",
              "0    0.452553\n",
              "Name: Label, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbgSVMIyuxeA"
      },
      "source": [
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['Text'], df['Label'], \n",
        "                                                                    random_state=2018, \n",
        "                                                                    test_size=0.2, \n",
        "                                                                    stratify=df['Label'])\n",
        "\n",
        "# Using temp_text and temp_labels to create validation and test set\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.1, \n",
        "                                                                stratify=temp_labels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrIzvsJpu2UN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2019612f-bc6f-4726-95d3-16981e6b0aa1"
      },
      "source": [
        "# Importing BERT-base pretrained model\n",
        "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['roberta.embeddings.position_ids']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDK7qeXcu5BG",
        "outputId": "2bcda693-c3e9-4511-f5c5-5a62990d18e9"
      },
      "source": [
        "# Sample data\n",
        "text = [\"this is a bertweet model tutorial\", \"we will fine-tune a bertweet model\"]\n",
        "\n",
        "# Encode text\n",
        "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\n",
        "\n",
        "# Output\n",
        "print(sent_id)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [[101, 2023, 2003, 1037, 14324, 28394, 2102, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 28394, 2102, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "d_Z0bPs_vHFc",
        "outputId": "b6f073fb-704c-4564-82df-2b8834e39899"
      },
      "source": [
        "# Getting length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6fa77c3fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nBjyeM7vMVy"
      },
      "source": [
        "max_seq_len = 25\n",
        "# Tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# Tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# Tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL6K2gNuvRc2"
      },
      "source": [
        "# Converting Integer Sequences to Tensor\n",
        "\n",
        "# For train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "# For validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# For test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiHAXrtXvT9Z"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# Sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# DataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# Sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# DataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FapzQMuvWd5"
      },
      "source": [
        "# Freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcAr1VZ_vX7B"
      },
      "source": [
        "# Model Architecture\n",
        "\n",
        "class BERTweet_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bertweet):\n",
        "      \n",
        "      super(BERTweet_Arch, self).__init__()\n",
        "\n",
        "      self.bertweet = bertweet\n",
        "      \n",
        "      # Dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # Relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # Dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # Dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "      # Softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    # Define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      # Pass the inputs to the model  \n",
        "      _, cls_hs = self.bertweet(sent_id, attention_mask=mask)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # Output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # Apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oDoBoYdvay8"
      },
      "source": [
        "# Pass the pre-trained BERT to our define architecture\n",
        "model = BERTweet_Arch(bertweet)\n",
        "\n",
        "# Push the model to GPU\n",
        "model = model.to(device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDNCxGHnvdL1"
      },
      "source": [
        "# Optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmRnhX3Yv2Pc",
        "outputId": "eb77ff5b-a7ab-458b-b682-9ce2dcdec232"
      },
      "source": [
        "# Finding class weights\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Compute the class weights\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
        "\n",
        "print(class_wts)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.1045977 0.9134981]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tjhQmsGv4_e"
      },
      "source": [
        "# Convert class weights to tensor\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# Loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 10\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywnd2bQav7mS"
      },
      "source": [
        "# Function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # Empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # Iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # Progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # Push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # Clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # Get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # Compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # Add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # Backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # Append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # Compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # Predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # Reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  # Returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcOBTYhzv-jh"
      },
      "source": [
        "# Function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # Deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # Empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # Iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # Push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # Deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # Model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # Compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # Compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # Reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkVkTfzrwEPc",
        "outputId": "6a37abf7-01fb-48f0-f25b-9f500d6940f4"
      },
      "source": [
        "# Set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# Empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "# For each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    # Train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    # Evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    # Save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # Append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of     91.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.698\n",
            "Validation Loss: 0.678\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of     91.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.682\n",
            "Validation Loss: 0.688\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of     91.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.676\n",
            "Validation Loss: 0.663\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of     91.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.664\n",
            "Validation Loss: 0.666\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of     91.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.666\n",
            "Validation Loss: 0.665\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of     91.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.667\n",
            "Validation Loss: 0.652\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of     91.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.658\n",
            "Validation Loss: 0.646\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of     91.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.653\n",
            "Validation Loss: 0.664\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of     91.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.658\n",
            "Validation Loss: 0.650\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of     91.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.660\n",
            "Validation Loss: 0.660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRxyA2YhwGxX",
        "outputId": "a15dbfb2-89ee-49d9-f951-6521ea96ba06"
      },
      "source": [
        "# Load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyilS2rAwWAJ"
      },
      "source": [
        "# Get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfnj-b-wwYyJ",
        "outputId": "28614b8b-06c3-4e8d-a5ba-796af90508a5"
      },
      "source": [
        "# Model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.45      0.52        33\n",
            "           1       0.62      0.75      0.68        40\n",
            "\n",
            "    accuracy                           0.62        73\n",
            "   macro avg       0.61      0.60      0.60        73\n",
            "weighted avg       0.61      0.62      0.61        73\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "uLipc1l2wazm",
        "outputId": "8d9330cb-4fe7-44b3-b1a3-9a34ebc8c9c3"
      },
      "source": [
        "# Confusion matrix\n",
        "pd.crosstab(test_y, preds)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0   0   1\n",
              "row_0        \n",
              "0      15  18\n",
              "1      10  30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrVOeQPIwdse",
        "outputId": "5de63e95-3d13-4d05-d3de-4f3b22288e70"
      },
      "source": [
        "print (type(test_y))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFugKyQ0zx6r",
        "outputId": "b8cf5612-d22c-4d1d-c1ca-80bf7ff40f43"
      },
      "source": [
        "print (test_y)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
            "        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
            "        1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE7trlzk1Dml",
        "outputId": "31fa0c3e-eb33-485e-c249-de5e059e67ad"
      },
      "source": [
        "testy_list = test_y.tolist()\n",
        "print (testy_list)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfMjbpax1yIb",
        "outputId": "ba679da8-03f0-45de-ebf0-848904f9e65e"
      },
      "source": [
        "preds_list = preds.tolist()\n",
        "print (preds_list)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOQ-kdB414o6",
        "outputId": "7ce4fc71-c6d8-4741-97e1-a703079b5c4f"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "matrix = confusion_matrix(testy_list, preds_list)\n",
        "\n",
        "#Printing per class accuracy\n",
        "print (100*matrix.diagonal()/matrix.sum(axis=1))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[45.45454545 75.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV30XrYH6Xhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5fe865-847e-4813-c997-21cc368c60cd"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(testy_list, preds_list)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6164383561643836"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF8h5Y3yqrTA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}