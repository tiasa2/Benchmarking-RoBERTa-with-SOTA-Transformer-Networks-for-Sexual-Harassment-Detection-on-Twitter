# Repo for the paper

### List of transformers:


BERT - Done


RoBERTa - Done 


DistilBERT - Done


ALBERT - Done


T5 - Done


Ernie (On RobertaBaseCased and BERTBaseCased) and Baseline - Done


XLNet - Done


BERTweet - Done


BigBird - Done


Electra - Done


Longformer - Done (keep it as backup as longformer is mostly used for >1000 token input)


GPT - ( backup )Ashima


Reformer - Discard



Cite this for sarcasm dataset:

@article{misra2019sarcasm,
  title={Sarcasm Detection using Hybrid Neural Network},
  author={Misra, Rishabh and Arora, Prahal},
  journal={arXiv preprint arXiv:1908.07414},
  year={2019}
}

@book{book,
author = {Misra, Rishabh and Grover, Jigyasa},
year = {2021},
month = {01},
pages = {},
title = {Sculpting Data for ML: The first act of Machine Learning},
isbn = {978-0-578-83125-1}
}
